{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker recognition\n",
    "based on https://m.habr.com/ru/post/144491/\n",
    "\n",
    "Mozilla Common Voice (ru) https://voice.mozilla.org/ru/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.feature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.simplefilter('error')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка и обработка аудио\n",
    "\n",
    "num_voices = 55\n",
    "\n",
    "def extract_mfcc(path, sampling_rate=24000, shift=32., L=128., mel_coefs=120, mfcc_coefs=12, alpha=0.9, eps=1e-9):\n",
    "    '''\n",
    "    Transfroms a sound wave into a sequence of MFCC coefficients.\n",
    "    :param sound: 1D np.array with sound wave\n",
    "    :param sampling rate: number of points sampled per second\n",
    "    :param shift: difference between starting points of consecutive frames, ms.\n",
    "    :param L: window length, ms.\n",
    "    Returns 2D array of size (frames x mfcc_dim)\n",
    "    '''\n",
    "    sound, _ = librosa.load(path, sr=sample_rate) \n",
    "    #sound = librosa.util.normalize(sound)\n",
    "    mfcc = librosa.feature.mfcc(sound, sampling_rate, \n",
    "                                n_mfcc = mfcc_coefs)\n",
    "    energy = librosa.feature.rms(sound)\n",
    "    mfcc = np.vstack((mfcc,energy))\n",
    "    delta1 = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(delta1)\n",
    "    mfcc = np.vstack((mfcc,delta1,delta2))\n",
    "    return mfcc.T\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "num_frames = 64 #\n",
    "\n",
    "# это надо исправить под наши данные\n",
    "for voice in range(num_voices):\n",
    "    names = os.listdir(path='./'+str(voice))\n",
    "    for name in name:\n",
    "        print(voice, name, mfcc.shape)\n",
    "        mfcc = extract_mfcc(path+'/'+name)\n",
    "        if len(mfcc) < num_frames:\n",
    "            continue\n",
    "        if len(mfcc) > num_frames:\n",
    "            mfcc = mfcc[:num_frames]\n",
    "        X = X + [extract_mfcc(path+'/'+name)]\n",
    "        y = y + [voice]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация\n",
    "\n",
    "num_clasters = num_voices * 4   # этого достаточно для классификации дикторов: \n",
    "                                # SPEAKER IDENTIFICATION USING MEL FREQUENCY CEPSTRAL COEFFICIENTS\n",
    "clf = KMeans(num_clasters).fit(X_train, y_train)\n",
    "y_pred = clf.predict()\n",
    "\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализация\n",
    "# from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "\n",
    "reduced_data = PCA(n_components = 2).fit_transform(X_train)\n",
    "reduces_clf = KMeans(num_clusters).fit(reduced_data, y_train)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = reduced_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
