{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import librosa\n",
    "from librosa.display import specshow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct, ifft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "from collections import defaultdict, namedtuple\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# warnings.simplefilter('error')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './wav/'\n",
    "SAMPLE_RATE = 16000\n",
    "INPUT_SIZE = 39\n",
    "NUM_LAYERS = 1\n",
    "NUM_FRAMES = 63000\n",
    "TRAIN_SPLIT = 0.8\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 1\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(sound, sampling_rate=SAMPLE_RATE, shift=32., L=128., mel_coefs=120, mfcc_coefs=12, alpha=0.9, eps=1e-9):\n",
    "    mfcc = librosa.feature.mfcc(y=sound, sr=sampling_rate, n_mfcc=mfcc_coefs)\n",
    "    mfcc_energy = np.vstack((mfcc, (mfcc ** 2).sum(axis=0).reshape(1, -1)))\n",
    "    dx = librosa.feature.delta(mfcc_energy, order=1, width=3)\n",
    "    d2x = librosa.feature.delta(mfcc_energy, order=2, width=3)\n",
    "    res_features = np.vstack((mfcc_energy, dx, d2x)).T\n",
    "    return res_features.astype(np.float32)\n",
    "\n",
    "\n",
    "def show(audio):\n",
    "    return Audio(audio, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path=DATA_PATH):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    for uid, user_id in enumerate(os.listdir(data_path)):\n",
    "        videos = os.listdir(os.path.join(data_path, user_id))\n",
    "        random.shuffle(videos)\n",
    "        split = round(TRAIN_SPLIT * len(videos))\n",
    "        train_data[uid] = []\n",
    "        test_data[uid] = []\n",
    "        for vid, video_id in enumerate(videos):\n",
    "            for file in os.listdir(os.path.join(data_path, user_id, video_id)):\n",
    "                full_path = os.path.join(data_path, user_id, video_id, file)\n",
    "                record = librosa.load(full_path, sr=SAMPLE_RATE)[0]\n",
    "                for i in range(0, len(record) - NUM_FRAMES + 1, NUM_FRAMES):\n",
    "                    x = record[i:i+NUM_FRAMES]\n",
    "                    if vid < split:\n",
    "                        train_data[uid].append(extract_mfcc(x))\n",
    "                    else:\n",
    "                        test_data[uid].append(extract_mfcc(x))\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Records(Dataset):\n",
    "    def __init__(self, records, pos=5, neg=5):\n",
    "        self.indices = []\n",
    "        self.target = []\n",
    "        self.data = records\n",
    "        for u, data in records.items():\n",
    "            for i, x in enumerate(data):\n",
    "                for _ in range(pos):\n",
    "                    pos_x = i\n",
    "                    while pos_x == i:\n",
    "                        pos_x = random.randrange(0, len(data))\n",
    "                    self.indices.append((u, i, u, pos_x))\n",
    "                    self.target.append(1.0)\n",
    "                for _ in range(neg):\n",
    "                    neg_u = u\n",
    "                    while neg_u == u or len(records[neg_u]) == 0:\n",
    "                        neg_u = random.randrange(0, len(records))\n",
    "                    neg_x = random.randrange(0, len(records[neg_u]))\n",
    "                    self.indices.append((u, i, neg_u, neg_x))\n",
    "                    self.target.append(0.0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        u, x, neg_u, neg_x = self.indices[index]\n",
    "        return (self.data[u][x], self.data[neg_u][neg_x]), self.target[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw, test_raw = get_data()\n",
    "train_data = Records(train_raw)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "test_data = Records(test_raw)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_SIZE, num_layers=NUM_LAYERS, lstm_units=128):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_units, num_layers=num_layers)\n",
    "        self.sim = nn.Linear(in_features=1, out_features=1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # x, y -- tensors of size (batch, frames, 39)\n",
    "        x_h, _= self.lstm(x)\n",
    "        y_h, _= self.lstm(y)\n",
    "        \n",
    "        x_emb = x_h[:, -1]\n",
    "        y_emb = y_h[:, -1]\n",
    "        \n",
    "        sim = self.sim(nn.functional.cosine_similarity(x_emb, y_emb).unsqueeze(1)).squeeze(1)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, test_loader, device, n_epochs=N_EPOCHS, log_dir='./logs'):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        k = 0\n",
    "        \n",
    "        for i, (records, labels) in enumerate(train_loader):\n",
    "            records_a, records_b = records\n",
    "            records_a = records_a.to(device)\n",
    "            records_b = records_b.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            outputs = model(records_a, records_b)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            k += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= k\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        k = 0\n",
    "        for i, (records, labels) in enumerate(test_loader):\n",
    "            records_a, records_b = records\n",
    "            records_a = records_a.to(device)\n",
    "            records_b = records_b.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(records_a, records_b)\n",
    "                loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "        train_loss /= k\n",
    "        \n",
    "        writer.add_scalar('simple/train', train_loss, epoch)\n",
    "        writer.add_scalar('simple/test', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion, train_loader, test_loader, device, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
